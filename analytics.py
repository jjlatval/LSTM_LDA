# This file is used to perform analysis on the text output generated by the
# neural network model
import collections
import nltk
import smart_open

sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')

# Load original text
orig_file = './data/alice.txt'
orig_text = smart_open.file_smart_open(orig_file, 'rb').read().decode('utf-8').lower()

# Create tokens from original text file
tokens = nltk.word_tokenize(orig_text)

text1 = smart_open.file_smart_open('./output/alice_dim=128_gamma=0.05.txt', 'rb').read().decode('utf-8').lower()
text2 = smart_open.file_smart_open('./output/alice_dim=128_gamma=0.20.txt', 'rb').read().decode('utf-8').lower()
text3 = smart_open.file_smart_open('./output/alice_dim=128_gamma=0.50.txt', 'rb').read().decode('utf-8').lower()
text4 = smart_open.file_smart_open('./output/alice_dim=128_gamma=0.95.txt', 'rb').read().decode('utf-8').lower()
text5 = smart_open.file_smart_open('./output/alice_dim=128_gamma=0.90_topic=10.txt', 'rb').read().decode('utf-8').lower()


#  Construct the unigram language model
def unigram(tokens):
    model = collections.defaultdict(lambda: 0.01)
    for f in tokens:
        try:
            model[f] += 1
        except KeyError:
            model[f] = 1
            continue
    for word in model:
        model[word] = model[word]/float(len(model))
    return model


#computes perplexity of the unigram model on a testset
def perplexity(testset, model):
    testset = testset.split()
    per = 1
    N = 0
    for word in testset:
        N += 1
        per = float(per) * (1/float(model[word]))
    per = pow(per, 1/float(N))
    return per


def compute_perplexity(fulltext=None):
    if fulltext:
        model = unigram(tokens)
        perplexity_list = list()
        for sentence in (sent_detector.tokenize(fulltext.strip())):
            perplexity_list.append(perplexity(sentence, model))
        perplexity_list = [value for value in perplexity_list if value < float('inf')]
        return sum(perplexity_list)/len(perplexity_list)  # Average sentence perplexity
    return None

model = unigram(tokens)
print compute_perplexity(fulltext=orig_text)
print compute_perplexity(fulltext=text1)
print compute_perplexity(fulltext=text2)
print compute_perplexity(fulltext=text3)
print compute_perplexity(fulltext=text4)
print compute_perplexity(fulltext=text5)